
<center>
    <h1 lang="en">Publications</h1>
    <!--<ul id="filter" text-align="center" list-style="inside" display="inline-block">
            <li><a href="#" data-val="all" class="current"><span lang="en">All</span></a></li>
            <li><a id="top-button" href="#" data-val="top"><span lang="en">Top</span></a></li>
            <li><a href="#" data-val="journal"><span lang="en">Journals</span></a></li>
            <li><a href="#" data-val="conference"><span lang="en">Conferences</span></a></li>
            <li><a href="#" data-val="workshop"><span lang="en">Workshops</span></a></li>
            <li><a href="#" data-val="thesis"><span lang="en">Thesis</span></a></li>
    </ul>-->
</center>
<div class="publications">
    
    <h3 class="conference"><span lang="en">2020</span></h3>
    <ul class="publist">
          <li class="conference">
            <div class="details" onmouseenter="document.getElementById('C-Flow').src='../../images/2020/C-Flow/cflow.gif';" onmouseleave="document.getElementById('C-Flow').src='../../images/2020/C-Flow/0.png';">
              <a href="../research/C-Flow/index.html"><img src=
              "../../images/2020/C-Flow/0.png" height="100" width="100" alt=
              "C-Flow: Conditional Generative Flow Models for Images and 3D Point Clouds" id="C-Flow" class="thumb"></a>
              <ul class="pub">
                <li class="pubtitle">C-Flow: Conditional Generative Flow Models for Images and 3D Point Clouds</li>
                  <li class="pubauthor">A. Pumarola, S. Popov, F. Moreno-Noguer and V. Ferrari</li> 
                <li class="pubbooktitle">Conference in Computer Vision and Pattern Recognition (CVPR), 2020. </li> 
                <li>
                 <div class="list_buttons">
                    <a class="bibtexLink" href="../research/C-Flow/index.html" >Project Page</a>
                    <a class="bibtexLink" href="https://arxiv.org/abs/1912.07009" target="_blank">PDF</a>
                    <a class="bibtexLink" onclick="return toggle('/publications/PumarolaCflow2020/bib',this)">Bibtex</a>
                  </div>
                </li>  
              </ul>
            </div>
            <div style="display: none;" class="bibtex" id="/publications/PumarolaCflow2020/bib">
              <pre>@inproceedings{pumarola2020c,
  title={C-Flow: Conditional Generative Flow Models for Images and 3D Point Clouds},
  author={Pumarola, Albert and Popov, Stefan and Moreno-Noguer, Francesc and Ferrari, Vittorio},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={7949--7958},
  year={2020}
}</pre>
            </div>
          </li>
        
        <li class="conference">
            <div class="details" onmouseenter="document.getElementById('ganhand_20').src='../../images/2020/GanHand/ganhand_small.gif';" onmouseleave="document.getElementById('ganhand_20').src='../../images/2020/GanHand/ganhand_small.png';">
              <a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Corona_GanHand_Predicting_Human_Grasp_Affordances_in_Multi-Object_Scenes_CVPR_2020_paper.pdf"  target="_blank"><img src=
              "../../images/2020/GanHand/ganhand_small.png" height="100" width="100" alt=
              "Context-aware Human Motion Prediction" id="ganhand_20" class="thumb"></a>
              <ul class="pub">
                <li class="pubtitle">GanHand: Predicting Human Grasp Affordances in Multi-Object Scenes</li>
                <li class="pubauthor">E. Corona, A. Pumarola, G. Alenyà, F. Moreno-Noguer, G. Rogez </li>
                <li class="pubbooktitle">Conference in Computer Vision and Pattern Recognition (CVPR), 2020.</li>
                <li><font color="red"><b>Oral</b></font></li>
                <li>
                  <div class="list_buttons">
                    <a class="bibtexLink" href="http://www.iri.upc.edu/people/ecorona/ganhand/" >Project Page</a>
                    <a class="bibtexLink" href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Corona_GanHand_Predicting_Human_Grasp_Affordances_in_Multi-Object_Scenes_CVPR_2020_paper.pdf" target="_blank">PDF</a>
                    <a class="bibtexLink" href="https://github.com/enriccorona/YCB_Affordance">Dataset</a>
<!--                    <a class="bibtexLink" onclick="return toggle('/publications/CoronaContextArxiv19/abs',this)"><span lang="en">Abstract</span></a>-->
                    <a class="bibtexLink" onclick="return toggle('/publications/CoronaGanHand20/bib',this)">Bibtex</a>
                  </div>
                </li>
              </ul>
            </div>
            <div style="display: none;" class="bibtex" id="/publications/CoronaGanHand20/bib">
              <pre>@article{corona2020gan,
  title={GanHand: Predicting Human Grasp Affordances in Multi-Object Scenes},
  author={Corona, Enric and Pumarola, Albert and Aleny{\`a}, Guillem and Moreno-Noguer, Francesc and Rogez, Gregory},
  journal={Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2020}
}</pre>
            </div>
          </li>
        
        
        
        <li class="conference">
            <div class="details" onmouseenter="document.getElementById('context_20').src='../../images/2020/ContextPred/context_pred_small.gif';" onmouseleave="document.getElementById('context_20').src='../../images/2020/ContextPred/context_pred_small.png';">
              <a href="https://arxiv.org/abs/1904.03419"  target="_blank"><img src=
              "../../images/2020/ContextPred/context_pred_small.png" height="100" width="100" alt=
              "Context-aware Human Motion Prediction" id="context_20" class="thumb"></a>
              <ul class="pub">
                <li class="pubtitle">Context-aware Human Motion Prediction</li>
                <li class="pubauthor">E. Corona, A. Pumarola, G. Alenyà, F. Moreno-Noguer </li>
                <li class="pubbooktitle">Conference in Computer Vision and Pattern Recognition (CVPR), 2020.</li>
                <li>
                  <div class="list_buttons">
                    <a class="bibtexLink" href="http://www.iri.upc.edu/people/ecorona/ca-hmp/" >Project Page</a>
                    <a class="bibtexLink" href="https://arxiv.org/abs/1904.03419" target="_blank">PDF</a>
<!--                    <a class="bibtexLink" onclick="return toggle('/publications/CoronaContextArxiv19/abs',this)"><span lang="en">Abstract</span></a>-->
                    <a class="bibtexLink" onclick="return toggle('/publications/CoronaContextArxiv19/bib',this)">Bibtex</a>
                  </div>
                </li>
              </ul>
            </div>
            <div style="display: none;" class="bibtex" id="/publications/CoronaContextArxiv19/bib">
              <pre>@article{corona2020context,
  title={Context-aware Human Motion Prediction},
  author={Corona, Enric and Pumarola, Albert and Aleny{\`a}, Guillem and Moreno-Noguer, Francesc},
  journal={Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2020}
}</pre>
            </div>
          </li>
         
        </ul>
    
    
    
    
    <h3 class="conference"><span lang="en">2019</span></h3>
            
    <ul class="publist">
          <li class="conference">
            <div class="details" onmouseenter="document.getElementById('3dpeople').src='../../images/2019/3DPeople/3dpeople_small.gif';" onmouseleave="document.getElementById('3dpeople').src='../../images/2019/3DPeople/3dpeople_small.jpg';">
              <a href="../research/3DPeople/index.html"><img src=
              "../../images/2019/3DPeople/3dpeople_small.jpg" height="100" width="100" alt=
              "3DPeople: Modeling the Geometry of Dressed Humans" id="3dpeople" class="thumb"></a>
              <ul class="pub">
                <li class="pubtitle">3DPeople: Modeling the Geometry of Dressed Humans</li>
                  <li class="pubauthor">A. Pumarola, J. Sanchez, G. Choi, A. Sanfeliu and F. Moreno-Noguer</li> 
                <li class="pubbooktitle">International Conference on Computer Vision (ICCV), 2019 </li> 
                <li>
                 <div class="list_buttons">
                    <a class="bibtexLink" href="../research/3DPeople/index.html" >Project Page</a>
                    <a class="bibtexLink" href="https://arxiv.org/abs/1904.04571" target="_blank">PDF</a>
                    <a class="bibtexLink" href="https://cv.iri.upc-csic.es">Dataset</a>
                    <a class="bibtexLink" href="https://github.com/albertpumarola/3DPeople-Dataset"><i class="fa fa-github"></i> Code</a>
                    <a class="bibtexLink" onclick="return toggle('/publications/Pumarola3DPeople2019/bib',this)">Bibtex</a>
                  </div>
                </li>  
              </ul>
            </div>
            <div style="display: none;" class="bibtex" id="/publications/Pumarola3DPeople2019/bib">
              <pre>@inproceedings{pumarola20193dpeople,
  title={{3DPeople: Modeling the Geometry of Dressed Humans}},
  author={Pumarola, Albert and Sanchez, Jordi and Choi, Gary and Sanfeliu, Alberto and Moreno-Noguer, Francesc},
  booktitle={International Conference on Computer Vision (ICCV)},
  year={2019}
}</pre>
            </div>
          </li>
          <li class="article">
            <div class="details" onmouseenter="document.getElementById('GANimation_ijcv').src='../../images/2019/GANimation/ganimation_small.gif';" onmouseleave="document.getElementById('GANimation_ijcv').src='../../images/2019/GANimation/ganimation_small.jpg';">
              <a href="../research/GANimation/index.html"><img src=
              "../../images/2019/GANimation/ganimation_small.jpg" height="100" width="100" alt=
              "GANimation: One-Shot Anatomically Consistent Facial Animation" id="GANimation_ijcv" class="thumb"></a>
              <ul class="pub">
                <li class="pubtitle">GANimation: One-Shot Anatomically Consistent Facial Animation    </li>
                <li class="pubauthor">A. Pumarola, A. Agudo, A. M. Martinez, A. Sanfeliu and F. Moreno-Noguer </li> 
                <li class="pubbooktitle">International Journal of Computer Vision (IJCV), 2019. </li>                <li>
                 <div class="list_buttons">
                    <a class="bibtexLink" href="../research/GANimation/index.html" >Project Page</a>
                    <a class="bibtexLink" href="https://rdcu.be/bPuaJ" target="_blank">PDF</a>
                    <a class="bibtexLink" href="https://github.com/albertpumarola/GANimation.git" target="_blank"><i class="fa fa-github"></i> Code</a>
                    <a class="bibtexLink" onclick="return toggle('/publications/PumarolaGanimationIJCV19/bib',this)">Bibtex</a>
                  </div>
                </li>
              </ul>
            </div>
            <div style="display: none;" class="bibtex" id="/publications/PumarolaGanimationIJCV19/bib">
              <pre>@article{Pumarola_ijcv2019, 
  title = {GANimation: One-Shot Anatomically Consistent Facial Animation}, 
  author = {A. Pumarola and A. Agudo and A.M. Martinez and A. Sanfeliu and F. Moreno-Noguer}, 
  booktitle = {International Journal of Computer Vision (IJCV)}, 
  volume = {}, 
  number = {}, 
  issn = {}, 
  pages = {}, 
  year = {2019} 
}</pre>
            </div>
          </li>
          <li class="conference">
            <div class="details" onmouseenter="document.getElementById('FaSTGAN').src='../../images/2019/FaSTGAN/fastgan_small.gif';" onmouseleave="document.getElementById('FaSTGAN').src='../../images/2019/FaSTGAN/fastgan_small.jpg';">
              <a href="../research/FaSTGAN/index.html"><img src=
              "../../images/2019/FaSTGAN/fastgan_small.jpg" height="100" width="100" alt=
              "Fast video object segmentation with Spatio-Temporal GANs" id="FaSTGAN" class="thumb"></a>
              <ul class="pub">
                <li class="pubtitle">Fast video object segmentation with Spatio-Temporal GANs</li>
                  <li class="pubauthor">S. Caelles*, A. Pumarola*, F. Moreno-Noguer, A. Sanfeliu and L. Van Gool <br><small><i>(*Equally contributed)</i></small> </li> 
                <li class="pubbooktitle">arXiv preprint arXiv:1903.12161, 2019. </li> 
                <li>
                 <div class="list_buttons">
                    <a class="bibtexLink" href="../research/FaSTGAN/index.html" >Project Page</a>
                    <a class="bibtexLink" href="https://arxiv.org/abs/1903.12161" target="_blank">PDF</a>
                    <a class="bibtexLink" onclick="return toggle('/publications/CaellesFaSTGAN2019/bib',this)">Bibtex</a>
                  </div>
                </li>
              </ul>
            </div>
            <div style="display: none;" class="bibtex" id="/publications/CaellesFaSTGAN2019/bib">
              <pre>@article{caelles2019fast,
  title={Fast video object segmentation with Spatio-Temporal GANs},
  author={Caelles, Sergi and Pumarola, Albert and Moreno-Noguer, Francesc and Sanfeliu, Alberto and Van Gool, Luc},
  journal={arXiv preprint arXiv:1903.12161},
  year={2019}
}</pre>
            </div>
          </li>
          
          <li class="chapter">
            <div class="details" onmouseenter="document.getElementById('releativeloc_springer').src='../../images/2019/RelativeLoc/pl-slam_small.gif';" onmouseleave="document.getElementById('releativeloc_springer').src='../../images/2019/RelativeLoc/pl-slam_small.png';">
              <a href="https://link.springer.com/chapter/10.1007/978-3-030-12945-3_17"  target="_blank"><img src=
              "../../images/2019/RelativeLoc/pl-slam_small.png" height="80" width="100" alt=
                                                               "Relative Localization for Aerial Manipulation with PL-SLAM" id="releativeloc_springer" class="thumb"></a>
              <ul class="pub">
                <li class="pubtitle">Relative Localization for Aerial Manipulation with PL-SLAM</li>
                <li class="pubauthor">A. Pumarola, A. Vakhitov, A. Agudo, F. Moreno-Noguer, A. Sanfeliu </li>
                <li class="pubbooktitle">Aerial Robotic Manipulation, Springer, 2019.</li>
                <li>
                  <div class="list_buttons">
                    <a class="bibtexLink" href="https://link.springer.com/chapter/10.1007/978-3-030-12945-3_17" target="_blank">PDF</a>
                    <a class="bibtexLink" onclick="return toggle('/publications/PumarolaRelativeSpringer19/bib',this)">Bibtex</a>
                  </div>
                </li>
              </ul>
            </div>
            <div style="display: none;" class="bibtex" id="/publications/PumarolaRelativeSpringer19/bib">
              <pre>@incollection{pumarola2019relative,
  title={Relative Localization for Aerial Manipulation with PL-SLAM},
  author={Pumarola, Albert and Vakhitov, Alexander and Agudo, Antonio and Moreno-Noguer, Francesc and Sanfeliu, Alberto},
  booktitle={Aerial Robotic Manipulation},
  pages={239--248},
  year={2019},
  publisher={Springer}
}</pre>
            </div>
          </li>
          <li class="chapter">
            <div class="details">
              <a href="https://link.springer.com/chapter/10.1007/978-3-030-12945-3_17"  target="_blank"><img src=
              "../../images/2019/CrawlerDet/small.png" height="65" width="100" alt=
              "Perception for Detection and Graspin" class="thumb"></a>
              <ul class="pub">
                <li class="pubtitle">Perception for Detection and Grasping</li>
                <li class="pubauthor">E. Guerra, A. Pumarola, A. Grau, A. Sanfeliu </li>
                <li class="pubbooktitle">Aerial Robotic Manipulation, Springer, 2019.</li>
                <li>
                  <div class="list_buttons">
                    <a class="bibtexLink" href="https://link.springer.com/chapter/10.1007/978-3-030-12945-3_20" target="_blank">PDF</a>
                    <a class="bibtexLink" onclick="return toggle('/publications/GuerraSpringuer19/bib',this)">Bibtex</a>
                  </div>
                </li>
              </ul>
            </div>
            <div style="display: none;" class="bibtex" id="/publications/GuerraSpringuer19/bib">
              <pre>@incollection{guerra2019perception,
  title={Perception for Detection and Grasping},
  author={Guerra, Edmundo and Pumarola, Albert and Grau, Antoni and Sanfeliu, Alberto},
  booktitle={Aerial Robotic Manipulation},
  pages={275--283},
  year={2019},
  publisher={Springer}
}</pre>
            </div>
          </li>
        </ul>
    
    
    
    
        <h3 class="conference"><span lang="en">2018</span></h3>
        <ul class="publist">
          <li class="conference">
            <div class="details" onmouseenter="document.getElementById('GANimation_eccv').src='../../images/2018/GANimation/GANimation_small.gif';" onmouseleave="document.getElementById('GANimation_eccv').src='../../images/2018/GANimation/GANimation_small.png';">
              <a href="../research/GANimation/index.html"><img src=
              "../../images/2018/GANimation/GANimation_small.png" height="100" width="100" alt=
              "GANimation: Anatomically-aware Facial Animation from a single image" id="GANimation_eccv" class="thumb"></a>
              <ul class="pub">
                <li class="pubtitle">GANimation: Anatomically-aware Facial Animation from a Single Image  </li>
                <li class="pubauthor">A. Pumarola, A. Agudo, A. M. Martinez, A. Sanfeliu and F. Moreno-Noguer </li> 
                <li class="pubbooktitle">European Conference on Computer Vision (ECCV), 2018. </li>
                <li><a href="https://eccv2018.org/awards-2/"  target="_blank"><font color="red"><b>Best Paper Award Honorable Mention</b></font></a></li> 
                <li>
                 <div class="list_buttons">
                    <a class="bibtexLink" href="../research/GANimation/index.html" >Project Page</a>
                    <a class="bibtexLink" href="https://arxiv.org/abs/1807.09251" target="_blank">PDF</a>
                    <a class="bibtexLink" href="https://github.com/albertpumarola/GANimation.git" target="_blank"><i class="fa fa-github"></i> Code</a>
<!--                    <a class="bibtexLink" onclick="return toggle('/publications/PumarolaGanimationECCV18/abs',this)"><span lang="en">Abstract</span></a>-->
                    <a class="bibtexLink" onclick="return toggle('/publications/PumarolaGanimationECCV18/bib',this)">Bibtex</a>
                  </div>
                </li>
              </ul>
            </div>
            <div style="display: none;" class="abstractbox" id="/publications/PumarolaGanimationECCV18/abs">
              <p>Recent advances in Generative Adversarial Networks (GANs) have shown remarkable improvements in the  task of facial expression synthesis. The most successful results are  obtained by StarGAN, an architecture that conditions GANs'  generation process with images of a specific domain, namely a set of images of persons sharing the same expression. While effective, this approach only allows generating a discrete number of expressions, determined by the  content of the dataset. In this paper, instead, we introduce a novel GAN conditioning scheme based on  Action Units (AU) annotations, which describe in a continuous manifold  the  anatomical facial  movements defining a human expression. Our approach permits controlling the magnitude of activation of each AU and combine several of them. Additionally, we propose a fully unsupervised strategy to train the model, that requires only images annotated with their activated AUs, and exploit attention mechanisms that make our network robust to changing backgrounds and illumination conditions. Extensive evaluation show that our approach goes beyond competing conditional generators both in the capability to synthesize a much wider range of expressions ruled by anatomically feasible muscle movements, as in the capacity of dealing with images in the wild. </p>
            </div>
            <div style="display: none;" class="bibtex" id="/publications/PumarolaGanimationECCV18/bib">
              <pre>@inproceedings{pumarola2018ganimation,
title={{GANimation: Anatomically-aware Facial Animation from a Single Image}},
author={A. Pumarola and A. Agudo and A.M. Martinez and A. Sanfeliu and F. Moreno-Noguer},
booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
year={2018}
}</pre>
            </div>
          </li>
          <li class="conference">
            <div class="details" onmouseenter="document.getElementById('person-synthesis_cvpr').src='../../images/2018/person-synthesis/person-synthesis_cvpr_small.gif';" onmouseleave="document.getElementById('person-synthesis_cvpr').src='../../images/2018/person-synthesis/person-synthesis_cvpr_small.jpg';">
              <a href="../research/person-synthesis/index.html"><img src=
              "../../images/2018/person-synthesis/person-synthesis_cvpr_small.jpg" height="100" width="100" alt=
              "Unsupervised Person Image Synthesis in Arbitrary Poses" id="person-synthesis_cvpr" class="thumb"></a>
              <ul class="pub">
                <li class="pubtitle">Unsupervised Person Image Synthesis in Arbitrary Poses</li>
                <li class="pubauthor">A. Pumarola, A. Agudo, A. Sanfeliu and F. Moreno-Noguer </li>
                <li class="pubbooktitle">Conference in Computer Vision and Pattern Recognition (CVPR), 2018. </li>
                <li><font color="red"><b>Spotlight</b></font></li>
                <li>
                 <div class="list_buttons">
                    <a class="bibtexLink" href="../research/person-synthesis/index.html" >Project Page</a>
                    <a class="bibtexLink" href="https://arxiv.org/abs/1809.10280" target="_blank">PDF</a>
<!--                    <a class="bibtexLink" onclick="return toggle('/publications/PumarolaPersonCVPR18/abs',this)"><span lang="en">Abstract</span></a>-->
                    <a class="bibtexLink" onclick="return toggle('/publications/PumarolaPersonCVPR18/bib',this)">Bibtex</a>
                  </div>
                </li>
              </ul>
            </div>
            <div style="display: none;" class="abstractbox" id="/publications/PumarolaPersonCVPR18/abs">
              <p>We present a novel approach for synthesizing photo-realistic images of people in arbitrary poses using generative adversarial learning. Given an input image of a person and a desired pose represented by a 2D skeleton, our model renders the image of the same person under the new pose, synthesizing novel views of the parts visible in the input image and hallucinating those that are not seen. This problem has recently been addressed in a supervised manner, i.e., during training the ground truth images under the new poses are given  to the network. We go beyond these approaches by proposing a fully unsupervised strategy. We tackle this challenging scenario by splitting the problem into two principal subtasks.  First, we consider a pose conditioned bidirectional generator that maps back the initially rendered image to the original pose, hence being directly comparable to the input image without the need to resort to any training image. Second, we devise a novel loss function that incorporates content and style terms, and aims at producing images of high perceptual quality. Extensive experiments conducted on the DeepFashion dataset demonstrate that the images rendered by our model are very close in appearance to those obtained by fully supervised approaches.</p>
            </div>
            <div style="display: none;" class="bibtex" id="/publications/PumarolaPersonCVPR18/bib">
              <pre>@inproceedings{pumarola2018unsupervised,
title={{Unsupervised Person Image Synthesis in Arbitrary Poses}},
author={A. Pumarola and A. Agudo and A. Sanfeliu and F. Moreno-Noguer},
booktitle={Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)},
year={2018}
}</pre>
            </div>
          </li>
          <li class="conference">
            <div class="details" onmouseenter="document.getElementById('deformnet_cvpr').src='../../images/2018/DeformNet/deformnet_cvpr_small.gif';" onmouseleave="document.getElementById('deformnet_cvpr').src='../../images/2018/DeformNet/deformnet_cvpr_small.png';">
              <a href="../research/DeformNet/index.html"><img src=
              "../../images/2018/DeformNet/deformnet_cvpr_small.png" height="100" width="100" alt=
              "Geometry-Aware Network for Non-Rigid Shape Prediction from a Single View" id="deformnet_cvpr" class="thumb"></a>
              <ul class="pub">
                <li class="pubtitle">Geometry-Aware Network for Non-Rigid Shape Prediction from a Single View</li>
                <li class="pubauthor">A. Pumarola, A. Agudo, L. Porzi, A. Sanfeliu, V. Lepetit and F. Moreno-Noguer </li>
                <li class="pubbooktitle">Conference in Computer Vision and Pattern Recognition (CVPR), 2018.</li>
                <li>
                  <div class="list_buttons">
                    <a class="bibtexLink" href="../research/DeformNet/index.html" >Project Page</a>
                    <a class="bibtexLink" href="https://arxiv.org/abs/1809.10305" target="_blank">PDF</a>
<!--                    <a class="bibtexLink" onclick="return toggle('/publications/PumarolaDeformCVPR18/abs',this)"><span lang="en">Abstract</span></a>-->
                    <a class="bibtexLink" onclick="return toggle('/publications/PumarolaDeformCVPR18/bib',this)">Bibtex</a>
                  </div>
                </li>
              </ul>
            </div>
            <div style="display: none;" class="abstractbox" id="/publications/PumarolaDeformCVPR18/abs">
              <p>We propose a method for predicting the 3D shape of a deformable surface from a single view. By contrast with previous approaches, we do not need a pre-registered template of the surface, and our method is robust to the lack of texture and partial occlusions. At the core of our approach is a {\it geometry-aware} deep architecture that tackles the problem as usually done in analytic solutions: first perform 2D detection of the mesh and then estimate a 3D shape that is geometrically consistent with the image. We train this architecture in an end-to-end manner using a large dataset of synthetic renderings of shapes under different levels of deformation, material properties, textures and lighting conditions. We evaluate our approach on a test split of this dataset and available real benchmarks, consistently improving state-of-the-art solutions with a significantly lower computational time.</p>
            </div>
            <div style="display: none;" class="bibtex" id="/publications/PumarolaDeformCVPR18/bib">
              <pre>@inproceedings{pumarola2018geometry,
title={{Geometry-Aware Network for Non-Rigid Shape Prediction from a Single View}},
author={A. Pumarola and A. Agudo and L. Porzi and A. Sanfeliu and V. Lepetit and F. Moreno-Noguer},
booktitle={Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)},
year={2018}
}</pre>
            </div>
          </li>
        </ul>
        <h3 class="conference"><span lang="en">2017</span></h3>
        <ul class="publist">
          <li class="conference">
            <div class="details" onmouseenter="document.getElementById('pl-slam_icra').src='../../images/2017/pl-slam/pl-slam_icra_small.gif';" onmouseleave="document.getElementById('pl-slam_icra').src='../../images/2017/pl-slam/pl-slam_icra.jpg';">
              <a href="../research/pl-slam/index.html"><img src="../../images/2017/pl-slam/pl-slam_icra.jpg" height="110" width="110" alt=
              "Real-Time Monocular Visual SLAM with Points and Lines" id="pl-slam_icra" class="thumb"></a>
              <ul class="pub" >
                <li class="pubtitle">PL-SLAM: Real-Time Monocular Visual SLAM with Points and Lines</li>
                <li class="pubauthor">A. Pumarola, A. Vakhitov, A. Agudo, A. Sanfeliu and F. Moreno-Noguer </li>
                <li class="pubbooktitle">IEEE International Conference on Robotics and Automation (ICRA), 2017.</li>
                <li>
                  <div class="list_buttons">
                    <a class="bibtexLink" href="../research/pl-slam/index.html" >Project Page</a>
                    <a class="bibtexLink" href="../../publications/files/PLSLAM_ICRA17.pdf" target="_blank">PDF</a>
<!--                    <a class="bibtexLink" onclick="return toggle('/publications/PumarolaICRA2017/abs',this)"><span lang="en">Abstract</span></a> -->
                    <a class="bibtexLink" onclick="return toggle('/publications/PumarolaICRA2017/bib',this)">Bibtex</a>
                  </div>
                </li>
              </ul>
            </div>
            <div style="display: none;" class="abstractbox" id="/publications/PumarolaICRA2017/abs">
              <p>Low textured scenes are well known to be  one of the main Achilles heels of geometric computer vision algorithms relying on point correspondences, and in particular for visual SLAM. Yet, there are many environments in which, despite being low textured, one can still reliably estimate  line-based geometric primitives, for instance in  city and  indoor scenes, or in the so-called ``Manhattan worlds'', where structured edges are predominant. In this paper we propose a solution to handle these situations. Specifically, we build upon ORB-SLAM, presumably the  current state-of-the-art solution both in terms of accuracy as efficiency, and extend its formulation to simultaneously handle both point and line correspondences. We propose a solution that can even work when most of the  points are vanished out from the input images, and, interestingly it can be initialized from solely the detection of line correspondences in three consecutive frames. We thoroughly evaluate our approach and the new initialization strategy on the TUM RGB-D benchmark and demonstrate that the use of lines does not only improve the performance of the original ORB-SLAM solution in poorly textured frames, but also systematically improves it in sequence frames combining points and lines, without compromising the efficiency.</p>
            </div>
            <div style="display: none;" class="bibtex" id="/publications/PumarolaICRA2017/bib">
              <pre>@inproceedings{pumarola2017plslam,
title={{PL-SLAM: Real-Time Monocular Visual SLAM with Points and Lines}},
author={A. Pumarola and A. Vakhitov and A. Agudo and A. Sanfeliu and F. Moreno-Noguer},
booktitle={International Conference in Robotics and Automation},
year={2017}
}</pre>
            </div>
          </li>
<!--        </ul>-->    
          <h3 class="journal"><span lang="en">Undergrad</span></h3>
<!--        <ul class="publist">-->
          <li class="journal">
            <div class="details" onmouseenter="document.getElementById('slam15').src='../../images/2015/soar/soar_small.gif';" onmouseleave="document.getElementById('slam15').src='../../images/2015/soar/soar.jpeg';">
              <a href="http://web.eecs.umich.edu/~soar/sitemaker/docs/pubs/Article_SOAR_GPSR-2.pdf" target="_blank"><img src=
              "../../images/2015/soar/soar.jpeg" height="100" width="100" alt=
              "Using a cognitive architecture for general purpose service robot control" id="slam15" class="thumb"></a>
              <ul class="pub">
                <li class="pubtitle">Using a cognitive architecture for general purpose service robot control</li>
                <li class="pubauthor">JY Puigbo*, A. Pumarola*, C. Angulo and R. Tellez</li>
                <li class="pubbooktitle">Connection Science, 2015</li>
                <li>
                  <div class="list_buttons">
                    <a class="bibtexLink" href="http://web.eecs.umich.edu/~soar/sitemaker/docs/pubs/Article_SOAR_GPSR-2.pdf" >PDF</a>
                    <a class="bibtexLink" href="https://www.youtube.com/watch?v=piCqb2O7rtU" target="_blank">Video</a>
<!--                    <a class="bibtexLink" onclick="return toggle('/publications/puigbo2015using/abs',this)"><span lang="en">Abstract</span></a> -->
                    <a class="bibtexLink" onclick="return toggle('/publications/puigbo2015using/bib',this)">Bibtex</a>
                  </div>
                </li>
              </ul>
            </div>
            <div style="display: none;" class="abstractbox" id="/publications/puigbo2015using/abs">
              <p>A humanoid service robot equipped with a set of simple action skills including navigating,
                    grasping, recognizing objects or people, among others, is considered in this paper. By using
                    those skills the robot should complete a voice command expressed in natural language encoding
                    a complex task (defined as the concatenation of a number of those basic skills). As a
                    main feature, no traditional planner has been used to decide skills to be activated, as well as
                    in which sequence. Instead, the SOAR cognitive architecture acts as the reasoner by selecting
                    which action the robot should complete, addressing it towards the goal. Our proposal allows
                    to include new goals for the robot just by adding new skills (without the need to encode new
                    plans). The proposed architecture has been tested on a human size humanoid robot, REEM,
                    acting as a general purpose service robot.</p>
            </div>
            <div style="display: none;" class="bibtex" id="/publications/puigbo2015using/bib">
              <pre>@article{puigbo2015using,
  title={Using a cognitive architecture for general purpose service robot control},
  author={Puigbo, Jordi-Ysard and Pumarola, Albert and Angulo, Cecilio and Tellez, Ricardo},
  journal={Connection Science},
  volume={27},
  number={2},
  pages={105--117},
  year={2015},
  publisher={Taylor \& Francis}
}</pre>
            </div>
          </li>
<!--        </ul>-->
<!--        <h3 class="workshop"><span lang="en">2014</span></h3>-->
<!--        <ul class="publist">-->
          <li class="workshop">
            <div class="details" onmouseenter="document.getElementById('slam14').src='../../images/2014/soar/soar_small.gif';" onmouseleave="document.getElementById('slam14').src='../../images/2014/soar/soar.jpeg';">
              <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.402.5541&rep=rep1&type=pdf"  target="_blank"><img src=
              "../../images/2014/soar/soar.jpeg" height="100" width="100" alt=
              "Controlling a General Purpose Service Robot By Means of a Cognitive Architecture."  id="slam14" class="thumb"></a>
              <ul class="pub">
                <li class="pubtitle">Controlling a General Purpose Service Robot By Means of a Cognitive Architecture</li>
                <li class="pubauthor">JY Puigbo, A. Pumarola and R. Tellez</li>
                <li class="pubbooktitle">AIC@ AI* IA, 2014</li>
                <li>
                  <div class="list_buttons">
                    <a class="bibtexLink" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.402.5541&rep=rep1&type=pdf" target="_blank">PDF</a>
                    <a class="bibtexLink" href="https://www.youtube.com/watch?v=piCqb2O7rtU" target="_blank">Video</a>
<!--                    <a class="bibtexLink" onclick="return toggle('/publications/puigbo2013controlling/abs',this)"><span lang="en">Abstract</span></a> -->
                    <a class="bibtexLink" onclick="return toggle('/publications/puigbo2013controlling/bib',this)">Bibtex</a>
                  </div>
                </li>
              </ul>
            </div>
            <div style="display: none;" class="abstractbox" id="/publications/puigbo2013controlling/abs">
              <p>In this paper, a humanoid service robot is equipped with
                    a set of simple action skills including navigating, grasping, recognizing
                    objects or people, among others. By using those skills the robot has to
                    complete a voice command in natural language that encodes a complex
                    task (defined as the concatenation of several of those basic skills). To decide
                    which of those skills should be activated and in which sequence no
                    traditional planner has been used. Instead, the SOAR cognitive architecture
                    acts as the reasoner that selects the current action the robot must
                    do, moving it towards the goal. We tested it on a human size humanoid
                    robot Reem acting as a general purpose service robot. The architecture
                    allows to include new goals by just adding new skills (without having to
                    encode new plans)</p>
            </div>
            <div style="display: none;" class="bibtex" id="/publications/puigbo2013controlling/bib">
              <pre>@inproceedings{puigbo2013controlling,
  title={Controlling a General Purpose Service Robot By Means of a Cognitive Architecture.},
  author={Puigbo, Jordi-Ysard and Pumarola, Albert and T{\'e}llez, Ricardo A},
  organization={Citeseer}
}</pre>
            </div>
          </li>
<!--        </ul>-->
      </div>
