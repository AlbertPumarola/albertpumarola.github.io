<html lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
  <meta name="keywords" content=
  "pumarola, albert, deep learning, computer vision, computer graphics, machine learning, artificial intelligence, IRI, slam, lines">
  <meta charset="UTF-8">
  <meta name="description" content=
  "Low textured scenes are well known to be  one of the main Achilles heels of geometric computer vision algorithms relying on point correspondences, and in particular for visual SLAM. Yet, there are many environments in which, despite being low textured, one can still reliably estimate  line-based geometric primitives, for instance in  city and  indoor scenes, or in the so-called ``Manhattan worlds'', where structured edges are predominant. In this paper we propose a solution to handle these situations. Specifically, we build upon ORB-SLAM, presumably the  current state-of-the-art solution both in terms of accuracy as efficiency, and extend its formulation to simultaneously handle both point and line correspondences. We propose a solution that can even work when most of the  points are vanished out from the input images, and, interestingly it can be initialized from solely the detection of line correspondences in three consecutive frames. We thoroughly evaluate our approach and the new initialization strategy on the TUM RGB-D benchmark and demonstrate that the use of lines does not only improve the performance of the original ORB-SLAM solution in poorly textured frames, but also systematically improves it in sequence frames combining points and lines, without compromising the efficiency.">
  <meta name="author" content="Albert Pumarola">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#0074D9">
  <title>Albert Pumarola - PL-SLAM</title>
  <link rel="stylesheet" type="text/css" href="../../../style.css" media="screen">
</head>
<body>
  <div id="page">
    <div id="menubar">
      <h2><a href="../../#page" lang="en">Home</a></h2>
      <h2><a href="../../#publications" lang="en">Publications</a></h2>
      <h2><a href="../../#projects" lang="en">Projects</a></h2>
    </div>
    <div id="main">
      <h1 lang="en">PL-SLAM</h1>
      <div id="project">
        <div id="authors" lang="en">
          <p><strong>Albert Pumarola</strong>, 
              <a href="https://sites.google.com/site/alexandervakhitov/">Alexander Vakhitov</a>, 
              <a href="http://www.iri.upc.edu/people/aagudo/">Antonio Agudo</a>,
              <a href="http://www.iri.upc.edu/people/sanfeliu/">Alberto Sanfeliu</a>,
              <a href="http://www.iri.upc.edu/people/fmoreno/">Francesc Moreno-Noguer</a>
          </p>
        </div>
        <center><img src="../../../images/2017/pl-slam/teaser.jpg" height="500" width="900" alt="PL-SLAM" class="teaser"></center>
        <div class="abstract" lang="en">
          <p>Low textured scenes are well known to be  one of the main Achilles heels of geometric computer vision algorithms relying on point correspondences, and in particular for visual SLAM. Yet, there are many environments in which, despite being low textured, one can still reliably estimate  line-based geometric primitives, for instance in  city and  indoor scenes, or in the so-called ``Manhattan worlds'', where structured edges are predominant. In this paper we propose a solution to handle these situations. Specifically, we build upon ORB-SLAM, presumably the  current state-of-the-art solution both in terms of accuracy as efficiency, and extend its formulation to simultaneously handle both point and line correspondences. We propose a solution that can even work when most of the  points are vanished out from the input images, and, interestingly it can be initialized from solely the detection of line correspondences in three consecutive frames. We thoroughly evaluate our approach and the new initialization strategy on the TUM RGB-D benchmark and demonstrate that the use of lines does not only improve the performance of the original ORB-SLAM solution in poorly textured frames, but also systematically improves it in sequence frames combining points and lines, without compromising the efficiency. </p>
        </div>
        <div id="content">
          <ul id="filter">
<!--            <li><a href="http://server_name:port/"><span lang="en">Online demo</span></a></li>-->
          </ul>
          <div lang="en">
            <h2 id="model"><a name="video">Spotlight Video</a></h2>
            <center>
               <div class="videocontainer">
                  <div class="videowrapper"> 
                     <iframe width="560" height="315" align="center" src="//www.youtube.com/embed/_1uDk8enoAQ" seamless="" allowfullscreen=""></iframe>
                  </div>  
               </div>
            </center>
          </div>
            <p>The video shows an evaluation of PL-SLAM and the new initialization strategy 
               on a TUM RGB-D benchmark sequence. The sequence selected is the same as the 
               one used to generate Figure 1 of the paper. </p>

                <p class="small">Legend:</p>
            <ul>
                <li>Map: estimated camera position (green box), camera key frames
               (blue boxes), point features (green points) and line features (red-blue endpoints)</li>
                <li>Current Frame: point features (green boxes) and line features (red-blue endpoints)</li>
            </ul>
          </div>
        </div>
        <h2><span lang="en">BibTex</span></h2>
        <div class="alert" lang="en">
            @incollection{pumarola2019relative,<br>
                &nbsp;&nbsp;&nbsp;&nbsp;title={Relative Localization for Aerial Manipulation with PL-SLAM},<br>
                &nbsp;&nbsp;&nbsp;&nbsp;author={A. Pumarola and A. Vakhitov and A. Agudo and F. Moreno-Noguer and A. Sanfeliu},<br>
                &nbsp;&nbsp;&nbsp;&nbsp;booktitle={Aerial Robotic Manipulation},<br>
                &nbsp;&nbsp;&nbsp;&nbsp;pages={239--248},<br>
                &nbsp;&nbsp;&nbsp;&nbsp;year={2019},<br>
                &nbsp;&nbsp;&nbsp;&nbsp;publisher={Springer}<br>
            }<br>
            <br>
            @inproceedings{pumarola2017plslam,<br>
                &nbsp;&nbsp;&nbsp;&nbsp;title={{PL-SLAM: Real-Time Monocular Visual SLAM with Points and Lines}},<br>
                &nbsp;&nbsp;&nbsp;&nbsp;author={A. Pumarola and A. Vakhitov and A. Agudo and A. Sanfeliu and F. Moreno-Noguer},<br>
                &nbsp;&nbsp;&nbsp;&nbsp;booktitle={International Conference in Robotics and Automation},<br>
                &nbsp;&nbsp;&nbsp;&nbsp;year={2017}<br>
            }
        </div>
        <h2><span lang="en">Publications</span></h2>
        <div class="publications">
        <h3 class="conference"><span lang="en">2017</span></h3>
        <ul class="publist">
          <li class="conference">
            <div class="details">
              <a href="../../publications/files/PLSLAM_ICRA17.pdf" target="_blank"><img src=
              "../../images/2017/pl-slam/thumbnail.png" height="167" width="130" alt=
              "Real-Time Monocular Visual SLAM with Points and Lines" id="pl-slam_icra" class="thumb"></a>
              <ul class="pub">
                <li class="pubtitle">PL-SLAM: Real-Time Monocular Visual SLAM with Points and Lines</li>
                <li class="pubauthor">A. Pumarola and A. Vakhitov and A. Agudo and A. Sanfeliu and F. Moreno-Noguer </li>
                <li class="pubbooktitle">IEEE International Conference on Robotics and Automation (ICRA), 2017.</li>
                <li>
                  <div class="list_buttons">
                    <a class="bibtexLink" href="../../publications/files/PLSLAM_ICRA17.pdf" target="_blank">PDF</a>
                    <a class="bibtexLink" onclick="return toggle('/publications/PumarolaICPR2017/abs',this)"><span lang="en">Abstract</span></a>
                    <a class="bibtexLink" onclick="return toggle('/publications/PumarolaICPR2017/bib',this)">Bibtex</a>
                  </div>
                </li>
              </ul>
            </div>
            <div style="display: none;" class="abstractbox" id="/publications/PumarolaICPR2017/abs">
              <p>Low textured scenes are well known to be  one of the main Achilles heels of geometric computer vision algorithms relying on point correspondences, and in particular for visual SLAM. Yet, there are many environments in which, despite being low textured, one can still reliably estimate  line-based geometric primitives, for instance in  city and  indoor scenes, or in the so-called ``Manhattan worlds'', where structured edges are predominant. In this paper we propose a solution to handle these situations. Specifically, we build upon ORB-SLAM, presumably the  current state-of-the-art solution both in terms of accuracy as efficiency, and extend its formulation to simultaneously handle both point and line correspondences. We propose a solution that can even work when most of the  points are vanished out from the input images, and, interestingly it can be initialized from solely the detection of line correspondences in three consecutive frames. We thoroughly evaluate our approach and the new initialization strategy on the TUM RGB-D benchmark and demonstrate that the use of lines does not only improve the performance of the original ORB-SLAM solution in poorly textured frames, but also systematically improves it in sequence frames combining points and lines, without compromising the efficiency.</p>
            </div>
            <div style="display: none;" class="bibtex" id="/publications/PumarolaICPR2017/bib">
              <pre>@inproceedings{pumarola2017plslam,
title={{PL-SLAM: Real-Time Monocular Visual SLAM with Points and Lines}},
author={A. Pumarola and A. Vakhitov and A. Agudo and A. Sanfeliu and F. Moreno-Noguer},
booktitle={International Conference in Robotics and Automation},
year={2017}
}</pre>
            </div>
          </li>
        </ul>
    

        <div id="content">
          <div lang="en">
            <h2 id="model">Acknowledgments</h2>
            <p> This work has been partially supported by the Spanish Ministry of Science and Innovation under projects HuMoUR TIN2017-90086-R and ColRobTransp DPI2016-78957; by the European project AEROARMS (H2020-ICT-2014-1-644271); by a Google faculty award; and by the Spanish State Research Agency through the Mar\'ia de Maeztu Seal of Excellence to IRI MDM-2016-0656. </p>
          </div>
        </div>
      </div>
    </div>
  </div>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js">
  </script> 
  <script>
  window.jQuery || document.write('<script src="../../../jquery.js"><\/script>')
  </script> 
  <script type="text/javascript" src="../../../site.js">
  </script> 
  <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-90616851-1', 'auto');
      ga('send', 'pageview');
  </script>
</body>
</html>
